# Web-Crawler

这里总结了尚硅谷的Python爬虫资料和我的练习代码。里面还包含一份Python基础的资料，只有面向对象以前的内容。
Python爬虫主要需要用到urllib和requests这两个库，requests是在urllib基础上开发的，更简单好用。
还需要学习xpath等解析插件，帮助我们在爬去网页所有信息之后，抓取出我们想要的部分信息。
为了应对反爬机制，还需要学习一下selenium,这个工具能够驱动浏览器向服务器发送请求，就像真实的用户在操作一下，避免被识别为爬虫程序。
最后，为了方便代码的编写，还需要学习一个scrapy爬虫框架,scrapy的工作原理如下。



爬虫的本质其实是模仿用户向服务器发送请求，然后接受返回的信息。通常来说一个页面上所有能看得见的信息都能够被获取，就算是用户登陆后的页面，
只要能够获取该用户的cookie，也能够伪装成该用户向服务器发送请求，并获取登入页面的信息。

网页上的信息都是能够被人工手动采集的，而爬虫的好处在于能够自动，大批量地获取网页信息。爬虫的代码并不难，难的是应对反爬机制。

常用的反爬机制有：
通过user-agent字段来反爬
利用代理ip反爬
通过cookies反爬
通过验证码反爬
动态页面的反爬

而这些反爬机制是能够被成功破解的，所以爬虫的难点就在于与反爬虫之间的博弈，即如何才能不被阻止获取网页信息。
